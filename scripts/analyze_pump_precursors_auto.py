#!/usr/bin/env python3
"""
Automated Pump Precursor Analysis
AI-powered analysis of signals before each pump by expert crypto analyst
"""

import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime, timedelta, timezone
import sys
import json
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config.settings import DATABASE

def get_db_connection():
    """Establish database connection"""
    return psycopg2.connect(**DATABASE, cursor_factory=RealDictCursor)

def load_pumps():
    """Load detected pumps from JSON file"""
    pumps_file = Path('/tmp/pump_analysis/pumps_found.json')

    if not pumps_file.exists():
        print("‚ùå –§–∞–π–ª —Å –ø–∞–º–ø–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ find_all_pumps.py")
        sys.exit(1)

    with open(pumps_file, 'r') as f:
        return json.load(f)

def get_signals_before_pump(conn, symbol, pump_time_ms, days_before=7):
    """
    Get all signals for a symbol in the period before pump
    """
    # Calculate time window
    pump_dt = datetime.fromtimestamp(pump_time_ms / 1000, tz=timezone.utc)
    window_start_dt = pump_dt - timedelta(days=days_before)

    window_start_ms = int(window_start_dt.timestamp() * 1000)

    query = """
    SELECT
        s.id,
        s.pair_symbol,
        s.signal_type,
        s.detected_at,
        s.signal_timestamp,
        s.spike_ratio_7d,
        s.spike_ratio_14d,
        s.spike_ratio_30d,
        s.signal_strength,
        s.baseline_7d,
        s.baseline_14d,
        s.baseline_30d,
        s.volume,
        s.price_at_signal,
        s.initial_confidence
    FROM pump.signals s
    WHERE s.pair_symbol = %s
      AND EXTRACT(EPOCH FROM s.signal_timestamp) * 1000 >= %s
      AND EXTRACT(EPOCH FROM s.signal_timestamp) * 1000 < %s
    ORDER BY s.signal_timestamp ASC
    """

    with conn.cursor() as cur:
        cur.execute(query, (symbol, window_start_ms, pump_time_ms))
        return cur.fetchall()

def analyze_signals_expert(pump, signals):
    """
    Expert analysis of precursor signals
    Analyzes patterns, timing, strength distribution
    """
    pump_time = datetime.fromtimestamp(pump['pump_start_time'] / 1000, tz=timezone.utc)
    pump_gain = float(pump['max_gain_24h'])

    if not signals:
        return {
            'conclusion': 'NO_SIGNALS',
            'summary': '–ü–∞–º–ø –ø—Ä–æ–∏–∑–æ—à–µ–ª –±–µ–∑ –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ –≤–Ω–µ—à–Ω–∏–π –∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä (–Ω–æ–≤–æ—Å—Ç–∏, –ª–∏—Å—Ç–∏–Ω–≥) –∏–ª–∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è.',
            'confidence': 'LOW',
            'pattern_type': 'SILENT_PUMP',
            'actionable': False
        }

    # Process signals
    pump_dt = datetime.fromtimestamp(pump['pump_start_time'] / 1000, tz=timezone.utc)

    by_type = {'FUTURES': [], 'SPOT': []}
    by_strength = {'EXTREME': [], 'STRONG': [], 'MEDIUM': [], 'WEAK': []}

    for sig in signals:
        sig_time = sig['signal_timestamp']
        if isinstance(sig_time, str):
            sig_time = datetime.fromisoformat(sig_time.replace('Z', '+00:00'))

        hours_before = (pump_dt - sig_time).total_seconds() / 3600
        sig['hours_before_pump'] = hours_before

        by_type[sig['signal_type']].append(sig)
        by_strength[sig['signal_strength']].append(sig)

    # Time distribution
    periods = {
        '0-24h': [s for s in signals if s['hours_before_pump'] <= 24],
        '24-48h': [s for s in signals if 24 < s['hours_before_pump'] <= 48],
        '48-72h': [s for s in signals if 48 < s['hours_before_pump'] <= 72],
        '72-120h': [s for s in signals if 72 < s['hours_before_pump'] <= 120],
        '120-168h': [s for s in signals if 120 < s['hours_before_pump'] <= 168],
    }

    # Calculate metrics
    total_signals = len(signals)
    futures_count = len(by_type['FUTURES'])
    spot_count = len(by_type['SPOT'])
    extreme_count = len(by_strength['EXTREME'])
    strong_count = len(by_strength['STRONG'])

    # Average spike ratios
    avg_spike_7d = sum(float(s['spike_ratio_7d']) for s in signals) / len(signals)
    max_spike_7d = max(float(s['spike_ratio_7d']) for s in signals)

    # Timing analysis
    signals_24h_before = len(periods['0-24h'])
    signals_48h_before = len(periods['24-48h'])

    # Build analysis
    analysis = {
        'total_signals': total_signals,
        'futures_count': futures_count,
        'spot_count': spot_count,
        'extreme_count': extreme_count,
        'strong_count': strong_count,
        'avg_spike_7d': round(avg_spike_7d, 1),
        'max_spike_7d': round(max_spike_7d, 1),
        'signals_24h_before': signals_24h_before,
        'signals_48h_before': signals_48h_before,
    }

    # Pattern detection
    if extreme_count >= 2 and signals_24h_before >= 1:
        pattern = 'STRONG_PRECURSOR'
        confidence = 'HIGH'
        actionable = True
        summary = f"–°–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {extreme_count} EXTREME —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –Ω–µ–¥–µ–ª—é, {signals_24h_before} –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24—á. " + \
                  f"–°—Ä–µ–¥–Ω–∏–π spike {avg_spike_7d:.1f}x. –ü–∞–º–ø –Ω–∞ +{pump_gain:.0f}% –±—ã–ª –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º."

    elif (extreme_count + strong_count) >= 3 and signals_48h_before >= 2:
        pattern = 'MEDIUM_PRECURSOR'
        confidence = 'MEDIUM'
        actionable = True
        summary = f"–£–º–µ—Ä–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {extreme_count + strong_count} —Å–∏–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –Ω–µ–¥–µ–ª—é. " + \
                  f"–≠—Å–∫–∞–ª–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞ 48—á –¥–æ –ø–∞–º–ø–∞ ({signals_48h_before} —Å–∏–≥–Ω–∞–ª–æ–≤). " + \
                  f"–ü–∞–º–ø +{pump_gain:.0f}% –∏–º–µ–ª –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏."

    elif futures_count > spot_count * 2 and signals_24h_before >= 1:
        pattern = 'FUTURES_LED'
        confidence = 'MEDIUM'
        actionable = True
        summary = f"FUTURES-–¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {futures_count} vs {spot_count} SPOT. " + \
                  f"–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ —Ñ—å—é—á–µ—Ä—Å–∞—Ö –ø—Ä–µ–¥—à–µ—Å—Ç–≤–æ–≤–∞–ª–∞ –ø–∞–º–ø—É. " + \
                  f"–í–æ–∑–º–æ–∂–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è."

    elif total_signals >= 5 and extreme_count == 0:
        pattern = 'WEAK_SIGNALS'
        confidence = 'LOW'
        actionable = False
        summary = f"–°–ª–∞–±—ã–µ —Å–∏–≥–Ω–∞–ª—ã: {total_signals} —Å–∏–≥–Ω–∞–ª–æ–≤ –±–µ–∑ EXTREME —É—Ä–æ–≤–Ω—è. " + \
                  f"–ü–∞–º–ø +{pump_gain:.0f}% –Ω–µ –±—ã–ª –æ—á–µ–≤–∏–¥–µ–Ω –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤. " + \
                  f"–í–æ–∑–º–æ–∂–Ω–æ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π —Ä–æ—Å—Ç –∏–ª–∏ –≤–Ω–µ—à–Ω–∏–π –∫–∞—Ç–∞–ª–∏–∑–∞—Ç–æ—Ä."

    elif signals_24h_before == 0 and total_signals > 0:
        pattern = 'EARLY_SIGNALS_ONLY'
        confidence = 'LOW'
        actionable = False
        summary = f"–†–∞–Ω–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã –±–µ–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å {total_signals} —Å–∏–≥–Ω–∞–ª–æ–≤ 3-7 –¥–Ω–µ–π –Ω–∞–∑–∞–¥, " + \
                  f"–Ω–æ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –ø–∞–º–ø–æ–º. –ü–∞—Ç—Ç–µ—Ä–Ω '–∑–∞—Ç—É—Ö–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ —à—Ç–æ—Ä–º–æ–º' - —Å–ª–æ–∂–Ω–æ —Ç–æ—Ä–≥–æ–≤–∞—Ç—å."

    else:
        pattern = 'UNCLEAR'
        confidence = 'LOW'
        actionable = False
        summary = f"–ù–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {total_signals} —Å–∏–≥–Ω–∞–ª–æ–≤ —Ä–∞–∑–Ω–æ–π —Å–∏–ª—ã. " + \
                  f"–ü–∞–º–ø +{pump_gain:.0f}% —Å–ª–æ–∂–Ω–æ –±—ã–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö."

    # Detailed breakdown
    time_analysis = []
    for period_name, period_sigs in periods.items():
        if period_sigs:
            extreme_in_period = len([s for s in period_sigs if s['signal_strength'] == 'EXTREME'])
            time_analysis.append({
                'period': period_name,
                'count': len(period_sigs),
                'extreme': extreme_in_period,
                'avg_spike': round(sum(float(s['spike_ratio_7d']) for s in period_sigs) / len(period_sigs), 1)
            })

    analysis.update({
        'pattern_type': pattern,
        'confidence': confidence,
        'actionable': actionable,
        'summary': summary,
        'time_breakdown': time_analysis
    })

    return analysis

def save_analysis_report(pump, signals, analysis, pump_idx):
    """Save individual pump analysis report"""
    reports_dir = Path('/tmp/pump_analysis/reports')
    reports_dir.mkdir(parents=True, exist_ok=True)

    # Sanitize filename
    symbol = pump['symbol'].replace('/', '_')
    pump_time = datetime.fromtimestamp(pump['pump_start_time'] / 1000, tz=timezone.utc)
    filename = f"{pump_idx:03d}_{symbol}_{pump_time.strftime('%Y%m%d_%H%M')}.json"

    report = {
        'pump_index': pump_idx,
        'symbol': pump['symbol'],
        'pump_time': pump_time.isoformat(),
        'pump_start_price': float(pump['start_price']),
        'max_gain_24h': float(pump['max_gain_24h']),
        'price_after_24h': float(pump['price_after_24h']),
        'signals_count': len(signals),
        'signals': [dict(s) for s in signals],
        'analysis': analysis
    }

    # Convert datetime objects to strings
    for sig in report['signals']:
        if 'signal_timestamp' in sig and not isinstance(sig['signal_timestamp'], str):
            sig['signal_timestamp'] = sig['signal_timestamp'].isoformat()
        if 'detected_at' in sig and not isinstance(sig['detected_at'], str):
            sig['detected_at'] = sig['detected_at'].isoformat()
        # Convert Decimal to float
        for key in sig:
            if sig[key] is not None and hasattr(sig[key], '__float__'):
                sig[key] = float(sig[key])

    report_file = reports_dir / filename
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)

    return report_file

def main():
    print("="*80)
    print("–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –°–ò–ì–ù–ê–õ–û–í-–ü–†–ï–î–í–ï–°–¢–ù–ò–ö–û–í –ü–ê–ú–ü–û–í")
    print("–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç –≤–µ–¥—É—â–µ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫—Ä–∏–ø—Ç–æ-—Ö–µ–¥–∂ —Ñ–æ–Ω–¥–∞")
    print("="*80)

    # Load pumps
    print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –ø–∞–º–ø–æ–≤...")
    pumps = load_pumps()
    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(pumps)} –ø–∞–º–ø–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    # Connect to database
    conn = get_db_connection()

    # Statistics
    pattern_stats = {
        'STRONG_PRECURSOR': 0,
        'MEDIUM_PRECURSOR': 0,
        'FUTURES_LED': 0,
        'WEAK_SIGNALS': 0,
        'EARLY_SIGNALS_ONLY': 0,
        'UNCLEAR': 0,
        'SILENT_PUMP': 0
    }

    actionable_pumps = []

    try:
        # Process each pump
        for idx, pump in enumerate(pumps, 1):
            pump_time = datetime.fromtimestamp(pump['pump_start_time'] / 1000, tz=timezone.utc)

            if idx % 50 == 0 or idx == 1:
                print(f"\n{'='*80}")
                print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {idx}/{len(pumps)} ({idx/len(pumps)*100:.1f}%%)")
                print(f"{'='*80}")

            print(f"\n[{idx}/{len(pumps)}] {pump['symbol']} | {pump_time} | +{float(pump['max_gain_24h']):.0f}%%")

            # Get signals before pump
            signals = get_signals_before_pump(
                conn,
                pump['symbol'],
                pump['pump_start_time'],
                days_before=7
            )

            # Expert analysis
            analysis = analyze_signals_expert(pump, signals)

            # Update stats
            pattern_stats[analysis['pattern_type']] += 1

            if analysis['actionable']:
                actionable_pumps.append({
                    'idx': idx,
                    'symbol': pump['symbol'],
                    'gain': float(pump['max_gain_24h']),
                    'confidence': analysis['confidence']
                })

            print(f"  üìä –°–∏–≥–Ω–∞–ª–æ–≤: {len(signals)} | –ü–∞—Ç—Ç–µ—Ä–Ω: {analysis['pattern_type']} | Confidence: {analysis['confidence']}")
            print(f"  üí° {analysis['summary']}")

            # Save report
            report_file = save_analysis_report(pump, signals, analysis, idx)

        # Final summary
        print("\n" + "="*80)
        print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print("="*80)

        print(f"\n–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(pumps)} –ø–∞–º–ø–æ–≤")
        print(f"–û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: /tmp/pump_analysis/reports/")

        print("\n" + "="*80)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–¢–¢–ï–†–ù–û–í:")
        print("="*80)
        for pattern, count in sorted(pattern_stats.items(), key=lambda x: -x[1]):
            pct = count / len(pumps) * 100
            print(f"  {pattern:25s}: {count:4d} ({pct:5.1f}%%)")

        print("\n" + "="*80)
        print(f"–ü–†–ò–ì–û–î–ù–´–ï –î–õ–Ø –¢–û–†–ì–û–í–õ–ò: {len(actionable_pumps)} –ø–∞–º–ø–æ–≤")
        print("="*80)

        if actionable_pumps:
            # Group by confidence
            high_conf = [p for p in actionable_pumps if p['confidence'] == 'HIGH']
            medium_conf = [p for p in actionable_pumps if p['confidence'] == 'MEDIUM']

            print(f"\nHIGH confidence: {len(high_conf)}")
            print(f"MEDIUM confidence: {len(medium_conf)}")

            avg_gain_actionable = sum(p['gain'] for p in actionable_pumps) / len(actionable_pumps)
            print(f"\n–°—Ä–µ–¥–Ω–∏–π gain –ø—Ä–∏–≥–æ–¥–Ω—ã—Ö –ø–∞–º–ø–æ–≤: +{avg_gain_actionable:.1f}%%")

        # Save summary
        summary_file = Path('/tmp/pump_analysis/analysis_summary.json')
        summary = {
            'total_pumps': len(pumps),
            'pattern_distribution': pattern_stats,
            'actionable_count': len(actionable_pumps),
            'actionable_pumps': actionable_pumps
        }

        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        print(f"\n‚úì –û–±—â–∞—è —Å–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_file}")

    finally:
        conn.close()

if __name__ == "__main__":
    main()
